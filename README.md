# Memory Recover

**Group 15 â€“ Yonghao Lin Â· Zhenqi Li Â· Yang Cao**

Memory Recover is a lightweight, privacy-preserving memory assistant that **listens**, **remembers**, and **reasons** entirely on-device. Running on a Raspberry Pi 5, it helps users recall moments (e.g. *â€œWhere did I leave my phone?â€*) without sending any audio or images to the cloud.

This is our demo video: [ğŸ¥ Click here to see our video](https://www.youtube.com/watch?v=puY44id-oYs&t=336s)
---

## Why Memory Recover?

| Problem | Our Solution |
|---------|--------------|
| **Memory decline** â€“ harder to recall events or item locations | Keyword-triggered snapshots + local retrieval |
| **Forgotten items** â€“ everyday objects misplaced | Multimodal retrieval-augmented generation (RAG) answers â€œwhere/whenâ€ queries |
| **Privacy concerns** â€“ cloud services risk data leaks | All processing (audio, vision, LLM, vector DB) runs locally |

---

## System Overview

![System Overview](./system_overview.jpg)

### 1 Â· Keyword Spotting  
Edge Impulse models (we have 2 models) detects (`hi man` Â· `take photo` Â· `unknown` Â· `noise`) and (`yes` Â· `no` Â· `unknown` Â· `noise`), waking the pipeline only when needed.


### 2 Â· â€œTake Photoâ€ Flow  
1. Capture image â†’ save to **`memory_images/`**  
2. User voice note â†’ Whisper â†’ text  
3. *Optional* VLM caption (on demand)  
4. Both texts â†’ embeddings â†’ **ChromaDB** (persisted locally)

### 3 Â· â€œHi Manâ€ Query Flow  
1. The user speaks a question (e.g., â€œWhere did I last see my phone?â€)  
2. The system converts the question into a vector and performs a semantic search in ChromaDB  
3. It retrieves top-k relevant memory entries (user notes + model captions)  
4. These retrieved memories and the original question are sent to the LLM (Llama 3.2) using a structured prompt  
5. The LLM reasons over the input to generate a natural-language answer and returns any referenced image paths  
6. The answer is spoken aloud, and related images are shown on screen

---

## Quick Start

> Tested on **Raspberry Pi 5** Â· 8 GB RAM Â· Python 3.11.2

```bash
git clone https://github.com/zl3508/Memory-Recover.git
cd memory-recover
source ~/ollama/bin/activate

# Core dependencies
pip install edge-impulse-linux

# Start Ollama and pull the model
ollama serve &
ollama pull llama3.2:3b

python mainthread.py
```

> **Tip:** set `DISPLAY=:0` for GUI pop-ups if using HDMI or VNC.

---

## Voice Commands

| Command       | Action |
|---------------|--------|
| **take photo** | Snapshot â†’ record description â†’ (optional) VLM processing |
| **hi man**     | Ask any memory question (e.g. â€œWhere was my wallet yesterday?â€) |
| **yes / no**   | Confirmation for re-recording or VLM captioning |

---

## Directory Layout

```
memory-recover/
â”œâ”€ mainthread.py          # entry point (interactive loop)
â”œâ”€ camera_capture.py
â”œâ”€ image_processing.py    # VLM captions
â”œâ”€ vector_store.py        # ChromaDB helpers
â”œâ”€ query_reasoning.py     # LLM prompts / answer object
â”œâ”€ memory_images/         # captured JPGs
â”œâ”€ chroma_db/             # persisted vectors
â”œâ”€ memory_text_user.json  # user notes
â””â”€ memory_text_model.json # VLM captions
```

---

